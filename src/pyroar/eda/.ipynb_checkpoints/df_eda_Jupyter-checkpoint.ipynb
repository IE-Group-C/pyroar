{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9eb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "DF Exploratory Data Analysis\n",
    "============================\n",
    "Easy and quick way to get the most important statistics for\n",
    "a dataframe with a single command showing several metrics.\n",
    "\"\"\"\n",
    "recomendation = {}\n",
    "recomendation.update(  {'Encoding': {} , \n",
    "                       'MissingValues': {} , \n",
    "                       'OutlierValuesDropLimit': '' ,\n",
    "                       'OutlierValuesDropList': {} ,\n",
    "                       'OutlierScaleList': {},\n",
    "                       'Scaling' : {}\n",
    "                       }\n",
    "                     )\n",
    "\n",
    "# rename df_eda to scan()\n",
    "def df_eda(df, dropThreshold=0.3, catsLblEncodeThreshold=20, catsDropThreashold=100, outlier_scale_lim = 1.5, outlier_scale_perc=0.005, outlier_drop_lim=3.0, pvalue=0.05):\n",
    "    \"\"\"\n",
    "    add documentation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    num_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat_features = df.select_dtypes(include=np.object).columns.tolist()\n",
    "    \n",
    "    \n",
    "    df = df\n",
    "    dropThreshold = dropThreshold\n",
    "    catsLblEncodeThreshold = catsLblEncodeThreshold\n",
    "    catsDropThreashold = catsDropThreashold\n",
    "    outlier_scale_lim = outlier_scale_lim\n",
    "    outlier_scale_perc = outlier_scale_perc\n",
    "    outlier_drop_lim = outlier_drop_lim\n",
    "    pvalue = pvalue\n",
    "    print(\"Head of dataframe:\\n\")\n",
    "    print(df.head())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nTail of dataframe:\\n\")\n",
    "    print(df.tail())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nNames of existing columns:\\n\")\n",
    "    print(df.columns)\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nData types of the existing columns:\\n\")\n",
    "    print(df.info())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nNumbers of columns with same data type:\\n\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nMemory usage of each column:\\n\")\n",
    "    print(df.memory_usage())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nShape of the dataframe (rows/columns):\\n\")\n",
    "    rows = df.shape[0]\n",
    "    cols = df.shape[1]\n",
    "    print(\"There are \" + str(rows) + \" rows and \" + str(cols) + \" columns in this dataframe.\")\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    print(\"\\nNumber of occuring duplicates in the dataframe:\\n\")\n",
    "    dupRows = df.duplicated().sum()\n",
    "    print(\"There are \" + str(dupRows) + \" duplicated rows in the dataframe.\")\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "\n",
    "    print(\"\\nNumber of occuring NULL/NA values per column:\\n\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nPercentage of occuring NULL/NA values per column:\\n\")\n",
    "    missing_perentage = df.isnull().sum()/df.isnull().count() * 100\n",
    "    print( missing_perentage)\n",
    "    for col, val in zip(df.columns , missing_perentage):\n",
    "         if(val >= dropThreshold):\n",
    "             # drop the column\n",
    "             print('Column: {} , recomendation: Drop Column.  Too many missing values {}%'.format(col, val))\n",
    "             recomendation['MissingValues'].update(  {col, 'DropColumn'})\n",
    "        \n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nNumber of occuring unique values per column:\\n\")\n",
    "    #### Note: This section should be applied to categorical Variables ONLY ************\n",
    "    n_unique = df.nunique()\n",
    "    print(\"\"+n_unique)\n",
    "    for col, val in zip(df.columns , n_unique):\n",
    "         if(val > catsDropThreashold):\n",
    "             # drop the column\n",
    "             print('Column: {} , recomendation: Drop.  Too many unique values {}'.format(col, val))\n",
    "             recomendation['Encoding'].update(  {col, 'DropColumn'})\n",
    "         elif val >catsLblEncodeThreshold:\n",
    "             # label encoder\n",
    "             print('Column: {} , recomendation: Use LabelEncoder.  Moderate number of unique values {}'.format(col, val))\n",
    "             recomendation['Encoding'].update(  {col, 'LabelEncoder'})\n",
    "         else:\n",
    "             #one hot encoding\n",
    "             print('Column: {} , recomendation: Use OneHotEncoder.  Limitted number of unique values {}'.format(col, val))\n",
    "             recomendation['Encoding'].update(  {col, 'OneHotEncoder'})\n",
    "        \n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "\n",
    "    #   add a section to check for outliers and make a recommandation:\n",
    "        #    1- great than 3IQR -> drop outlier values\n",
    "        #    2- greater than 1.5IQR and less than 3IQR -> robust scaling\n",
    "    \n",
    "    \n",
    "    ######################  IMPLEMENT OUTLIER DETECTION  ####################\n",
    "    num_out_low_lim = []\n",
    "    num_out_high_lim = []\n",
    "    for col in num_features:\n",
    "       num_out_low_lim[col], num_out_high_lim[col] =  count_outlier_numircal(col, num_out_low_lim, outlier_drop_lim)\n",
    "    \n",
    "    \n",
    "    recomendation['OutlierValuesDropLimit'].update(outlier_drop_lim )\n",
    "    ## Drop the outlier values that are greater than upper limit (e.g. 3 IQR )\n",
    "    for col, val in zip(num_features , num_out_high_lim):\n",
    "         if(val > 0 ):  \n",
    "             print('Column: {} , recomendation: Drop Outlier Value.  There are {} outlier values that are more than {} IQR'.format(col, val, outlier_drop_lim))\n",
    "             recomendation['OutlierValuesDropList'].update(  {col, 'DropValues'})\n",
    "         \n",
    "    ## use robustScalaer if the column has 3% or more of outliers that are more\n",
    "    ## the lower outliers limit (e.g. 1.5 IQR)\n",
    "    for col, val in zip(df.columns , num_out_low_lim):\n",
    "         if( (val/rows)  > outlier_scale_perc ):  # more than e.g 0.1% are outliers ==> Robust Scaler\n",
    "             print('Column: {} , recomendation: Use RobustScaler.  There are {:0.2f} % outlier values that are more than {} IQR'.format(col, (val/rows) , outlier_scale_lim))\n",
    "             recomendation['OutlierValuesDropList'].update(  {col, 'RobustScaler'})\n",
    "    \n",
    "    \n",
    "    print(\"\\nStandard statistics for each column:\\n\")\n",
    "    print(df.describe())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "       \n",
    "    print(\"\\nCorrelations between columns:\\n\")\n",
    "    print(df.corr())\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "    \n",
    "    \n",
    "    ########################### STATS #######################\n",
    "    import scipy\n",
    "    from scipy import stats\n",
    "    from scipy.stats import chisquare\n",
    "    import pandas as pd\n",
    "    \n",
    "    for var in num_features:\n",
    "        print(df[var])\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "        df[var] = df[var].fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## Stat_1. IMPLEMENT t-Test for numerical normal \n",
    "    \n",
    "    # get all numerical columns\n",
    "    # get the mean\n",
    "    # get the standard deviation\n",
    "    # get the z-score\n",
    "    # zscore = (actual_value - mean)/std\n",
    "    \n",
    "    #get the pvalue (pvalue: one sided p-value for zscore)\n",
    "    #pvalue = scipy.stats.norm.sf(abs(zscore))\n",
    "    \n",
    "    ### PROBLEM: we do not know the population mean\n",
    "    \n",
    "    # population_mean = 10  # it should be known but we do not have it\n",
    "    \n",
    "    # for col in num_features:\n",
    "    #     statistic, pval = t_stat, pval = stats.ttest_1samp(a = df[col],   # sample data   \n",
    "    #                popmean = population_mean)   # population mean\n",
    "        \n",
    "    #     if pval > 0.05:\n",
    "    #         print(\"Dataset has a normal distribution on var {}, as the null hyposthesis was not rejected for having a {:.4f} p-Value\".\n",
    "    #              format(col, pval))\n",
    "    #         recomendation['Scaling'].update(  {col, 'StandardScvaler'})\n",
    "    #     else:\n",
    "    #         print(\"Dataset df does not have a normal distribution on var {}, as the pvalue is {:.4f}\".format(col,pval))\n",
    "    #         recomendation['Scaling'].update(  {col, 'MinMaxScvaler'})\n",
    "    \n",
    "    \n",
    "    # To perform the Jarque-Bera goodness, first of all, it is needed to check that the sample is above 2000 (n>2000) data points.\n",
    "    # Otherwise it is recommended to use the Shapiro-Wilk tests to check for normal distribution of the sample.\n",
    "    \n",
    "    \n",
    "        ### shapiro test for normality\n",
    "        for col in num_features:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if rows <=2000: # use shapiro\n",
    "                statistic, pval = stats.shapiro(df[col])\n",
    "            else:   # use jarque_bera\n",
    "                statistic, pval = stats.jarque_bera(df[col])\n",
    "            \n",
    "            if pval > pvalue:\n",
    "                print(\"Dataset df has a normal distribution on var {}, as the null hyposthesis was not rejected for having a {:.4f} p-Value\".\n",
    "                     format(col, pval))\n",
    "                recomendation['Scaling'].update(  {col, 'StandardScaler'})\n",
    "            else:\n",
    "                print(\"Dataset df does not have a normal distribution on var {}, as the pvalue is {:.4f}\".\n",
    "                  format(col,pval))\n",
    "                recomendation['Scaling'].update(  {col, 'MinMaxScaler'})\n",
    "            \n",
    "    \n",
    "    ########################### Stat_2. IMPLEMENT poisson-Test if data not normal\n",
    "    # and starts from zero and above  (e.g. for count variables)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    ########################### Stat_3. IMPLEMENT ChaiSquare-Test for categorical nominal variables\n",
    "    ### PROBLEM: we do not have the population to perform Chai-Square test\n",
    "    \n",
    "       # Source: https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        for col in cat_features:\n",
    "            df[col] = label_encoder.fit_transform(myrdf[col])\n",
    "\n",
    "\n",
    "        from sklearn.feature_selection import chi2\n",
    "\n",
    "        X = df[cat_features]\n",
    "        y = df[target] # we have to ask the user to specify the target variable at the beginning and then make it binary 1,0\n",
    "\n",
    "        chi_scores = chi2(X,y)\n",
    "        \n",
    "        p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "        p_values.sort_values(ascending = False , inplace = True)\n",
    "        p_values<0.05\n",
    "        i = 0\n",
    "        for pval in p_values:\n",
    "            col = p_values.index[i]\n",
    "            if pval >= pvalue:   \n",
    "                print( \" The variable {} is recommended to be removed since it is independent from the target variable. We fail to reject the null hypothesis as p-value >= 5%: {} \".format(col,pval))\n",
    "                recomendation['Scaling'].update(  {col, '???????'}) #the recommendation is to drop the column. Also the 5% shouldn't be hard-coded\n",
    "            else:\n",
    "                print( \" The variable {} is recommended to be kept since it is dependent on the target variable. We reject the null hypothesis as p-value < 5%: {} \".format(col,pval))\n",
    "                recomendation['Scaling'].update(  {col, '??????'}) #the recommednation is to keep the column for ML. Also the 5% shouldn't be hard-coded  \n",
    "            i = i+1\n",
    "    '''for col in cat_features:\n",
    "        df[col] = pd.factorize(df[col])[0]\n",
    "        statistic, pval = chisquare( df[col].value_counts() / len(df[col]) )\n",
    "        if pval > pvalue:\n",
    "            print( \" The variable {} has a equal frequency. We fail to reject the null hypothesys as p-value >= 5%: {} \".format(col,pval))\n",
    "            recomendation['Scaling'].update(  {col, '???????'})\n",
    "        else:\n",
    "            print( \" The variable {} has not equal frequency. We reject the null hypothesys as p-value < 5% : {} \".format(col,pval))\n",
    "            recomendation['Scaling'].update(  {col, '??????'})'''\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    ########################### Stat_4. IMPLEMENT Binning for categorical ordinal variables to check normal distribuation\n",
    "    ### implement binning then\n",
    "    ##  t-test  ==> check for normality \n",
    "    ## reduce bins  ==> check for normality   till you get normal\n",
    "    \n",
    "    \n",
    "    #  \"step1:  set/change number of bins\n",
    "    #  if t-test:  \n",
    "    #    normal (ok) \n",
    "    #    store the number of bins in dictionary\n",
    "    # elif:\n",
    "    #    Go to step 1\n",
    "    \n",
    "    for col in num_features:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if rows <=2000: # use shapiro\n",
    "                statistic, pval = stats.shapiro(df[col])\n",
    "            else:   # use jarque_bera\n",
    "                statistic, pval = stats.jarque_bera(df[col])\n",
    "                if pval > pvalue:\n",
    "                        print(\"Dataset df has a normal distribution on var {}, as the null hyposthesis was not rejected for having a {:.4f} p-Value\".\n",
    "                                 format(col, pval))\n",
    "                            recomendation['Scaling'].update(  {col, 'StandardScaler'})\n",
    "                        else:\n",
    "                            print(\"Dataset df does not have a normal distribution on var {}, as the pvalue is {:.4f}\".\n",
    "                              format(col,pval))\n",
    "                            recomendation['Scaling'].update(  {col, 'MinMaxScaler'})\n",
    "    \n",
    "    \n",
    "    return recomendation\n",
    "    \n",
    "    \n",
    "    \n",
    "def count_outlier_numircal(x, limLow, limHigh):\n",
    "    # by Fahad\n",
    "    # ref https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
    "    #print(x)\n",
    "    import numpy as np\n",
    "    Q1 = np.percentile(x, 25,\n",
    "               interpolation = 'midpoint')\n",
    "     \n",
    "    Q3 = np.percentile(x, 75,\n",
    "               interpolation = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    \n",
    "    #Old_length=len(x)\n",
    "    #print(\"Old length: \", Old_length)\n",
    "     \n",
    "    # Upper bound for Exetreme outliers\n",
    "    upper_extreme = np.where(x >= (Q3+limHigh*IQR)).count()\n",
    "    # Lower bound for Exetreme outliers\n",
    "    lower_extreme = np.where(x <= (Q1-limHigh*IQR)).count()\n",
    "    \n",
    "    extreme_outliers = upper_extreme + lower_extreme\n",
    "    \n",
    "    \n",
    "    # Upper bound for Exetreme outliers\n",
    "    upper = np.where(x >= (Q3+limLow*IQR)).count()\n",
    "    # Lower bound for Exetreme outliers\n",
    "    lower = np.where(x <= (Q1-limLow*IQR)).count()\n",
    "    outliers_normal =  upper + lower - extreme_outliers\n",
    "    \n",
    "    return outliers_normal, extreme_outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
